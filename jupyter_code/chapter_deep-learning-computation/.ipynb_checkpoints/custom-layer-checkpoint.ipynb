{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d68ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义层：目前如果存在一个在深度学习框架中还不存在的层。 在这些情况下，你必须构建自定义层。\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "#构造一个没有任何参数的自定义层\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()#__init__()函数不进行任何初始化操作\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()#对X中每一个元素减去均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9d5327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2f311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将自定义层作为组件合并到更复杂的模型中\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d2eca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8626e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean() #当给网络发送随机数据后，检查输出Y均值是否为0。 由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c971b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义一个带参数的层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))#采用随机初始化参数，并由Parameter()函数包裹起来，目的是给这个参数一个名字标识\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd98fbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3891, -0.4105, -0.7189],\n",
       "        [-0.8655, -0.1785, -0.0529],\n",
       "        [-0.1446,  1.7960,  0.1220],\n",
       "        [ 0.4567,  0.0444, -0.5521],\n",
       "        [ 0.8643, -1.2224,  0.5013]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实例化MyLinear类并访问其模型权重参数\n",
    "linear = MyLinear(5, 3)#输入5维，输出3维的层\n",
    "linear.weight #随机初始化权重参数经过Parameter()函数包裹后的输出结果，多了一个\"Parameter containing\"名字标识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8d52a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0663, 0.0194],\n",
       "        [0.0000, 0.0000, 0.7687]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用自定义层直接执行前向传播计算\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c5dd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.9190],\n",
       "        [1.8329]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))#Sequential()包含两个自定义层\n",
    "net(torch.rand(2, 64))#给模型输入，得到输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb551249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1081, -0.0569,  0.0548, -0.1191, -0.0337, -0.0663, -0.2199,  0.0059,\n",
       "          0.0316,  0.2819],\n",
       "        [ 0.1398, -0.0566,  0.1458, -0.0881, -0.0742, -0.1665, -0.3488, -0.0111,\n",
       "         -0.0515,  0.2052]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25cc0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e65696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1864,  0.1115,  0.2045,  0.1158,  0.0509, -0.0844, -0.1204, -0.1001,\n",
       "          0.0161, -0.0837],\n",
       "        [-0.0844,  0.0268,  0.1926,  0.1999,  0.0818, -0.0431, -0.1353, -0.1442,\n",
       "          0.1123,  0.0215]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c797e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546ba1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0664, -0.1679,  0.0070, -0.0824, -0.0446,  0.0627,  0.1140,  0.1158,\n",
       "         -0.1583, -0.0231],\n",
       "        [-0.2058, -0.1821, -0.0210, -0.0889, -0.0383,  0.0144,  0.0755, -0.0767,\n",
       "         -0.0565,  0.0800]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "461dc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "            print(X)\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22fcc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1316, -0.2483, -0.3329,  0.0347, -0.1069, -0.0733, -0.0217, -0.1368,\n",
      "          0.3185,  0.0397, -0.1105,  0.0146,  0.0825,  0.1140, -0.0691, -0.0911,\n",
      "         -0.1115, -0.1111,  0.1636, -0.0972],\n",
      "        [ 0.0080, -0.4649, -0.4898,  0.0620,  0.1144,  0.0482, -0.0028, -0.2015,\n",
      "          0.1722,  0.1002,  0.0727,  0.0766, -0.0384,  0.2080, -0.0815, -0.1130,\n",
      "         -0.1624, -0.0376,  0.1681, -0.1492]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0658, -0.1241, -0.1665,  0.0173, -0.0535, -0.0367, -0.0109, -0.0684,\n",
      "          0.1593,  0.0199, -0.0553,  0.0073,  0.0413,  0.0570, -0.0346, -0.0455,\n",
      "         -0.0557, -0.0555,  0.0818, -0.0486],\n",
      "        [ 0.0040, -0.2324, -0.2449,  0.0310,  0.0572,  0.0241, -0.0014, -0.1007,\n",
      "          0.0861,  0.0501,  0.0363,  0.0383, -0.0192,  0.1040, -0.0407, -0.0565,\n",
      "         -0.0812, -0.0188,  0.0840, -0.0746]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.0329, -0.0621, -0.0832,  0.0087, -0.0267, -0.0183, -0.0054, -0.0342,\n",
      "          0.0796,  0.0099, -0.0276,  0.0037,  0.0206,  0.0285, -0.0173, -0.0228,\n",
      "         -0.0279, -0.0278,  0.0409, -0.0243],\n",
      "        [ 0.0020, -0.1162, -0.1225,  0.0155,  0.0286,  0.0120, -0.0007, -0.0504,\n",
      "          0.0431,  0.0251,  0.0182,  0.0191, -0.0096,  0.0520, -0.0204, -0.0283,\n",
      "         -0.0406, -0.0094,  0.0420, -0.0373]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.0164, -0.0310, -0.0416,  0.0043, -0.0134, -0.0092, -0.0027, -0.0171,\n",
      "          0.0398,  0.0050, -0.0138,  0.0018,  0.0103,  0.0143, -0.0086, -0.0114,\n",
      "         -0.0139, -0.0139,  0.0204, -0.0122],\n",
      "        [ 0.0010, -0.0581, -0.0612,  0.0077,  0.0143,  0.0060, -0.0003, -0.0252,\n",
      "          0.0215,  0.0125,  0.0091,  0.0096, -0.0048,  0.0260, -0.0102, -0.0141,\n",
      "         -0.0203, -0.0047,  0.0210, -0.0187]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1981, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af7cac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8884, -1.0308, -0.0831,  0.8758, -0.3174, -0.4683,  0.2343,  0.3336,\n",
      "         -1.5240, -0.3105,  0.1103, -0.6705, -0.3887,  0.9161, -0.1999,  0.3467,\n",
      "          0.3986,  0.2495, -0.3778, -0.7004],\n",
      "        [-0.8986, -1.0482, -0.0859,  0.8887, -0.3268, -0.4798,  0.2363,  0.3374,\n",
      "         -1.5472, -0.3220,  0.1037, -0.6756, -0.3956,  0.9269, -0.2069,  0.3607,\n",
      "          0.4110,  0.2602, -0.3910, -0.7050]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4442, -0.5154, -0.0416,  0.4379, -0.1587, -0.2342,  0.1172,  0.1668,\n",
      "         -0.7620, -0.1552,  0.0551, -0.3353, -0.1943,  0.4581, -0.0999,  0.1734,\n",
      "          0.1993,  0.1247, -0.1889, -0.3502],\n",
      "        [-0.4493, -0.5241, -0.0430,  0.4444, -0.1634, -0.2399,  0.1182,  0.1687,\n",
      "         -0.7736, -0.1610,  0.0518, -0.3378, -0.1978,  0.4635, -0.1035,  0.1804,\n",
      "          0.2055,  0.1301, -0.1955, -0.3525]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.2221, -0.2577, -0.0208,  0.2190, -0.0794, -0.1171,  0.0586,  0.0834,\n",
      "         -0.3810, -0.0776,  0.0276, -0.1676, -0.0972,  0.2290, -0.0500,  0.0867,\n",
      "          0.0997,  0.0624, -0.0945, -0.1751],\n",
      "        [-0.2246, -0.2620, -0.0215,  0.2222, -0.0817, -0.1200,  0.0591,  0.0844,\n",
      "         -0.3868, -0.0805,  0.0259, -0.1689, -0.0989,  0.2317, -0.0517,  0.0902,\n",
      "          0.1027,  0.0650, -0.0978, -0.1762]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.1111, -0.1288, -0.0104,  0.1095, -0.0397, -0.0585,  0.0293,  0.0417,\n",
      "         -0.1905, -0.0388,  0.0138, -0.0838, -0.0486,  0.1145, -0.0250,  0.0433,\n",
      "          0.0498,  0.0312, -0.0472, -0.0875],\n",
      "        [-0.1123, -0.1310, -0.0107,  0.1111, -0.0409, -0.0600,  0.0295,  0.0422,\n",
      "         -0.1934, -0.0403,  0.0130, -0.0845, -0.0494,  0.1159, -0.0259,  0.0451,\n",
      "          0.0514,  0.0325, -0.0489, -0.0881]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.0555, -0.0644, -0.0052,  0.0547, -0.0198, -0.0293,  0.0146,  0.0209,\n",
      "         -0.0953, -0.0194,  0.0069, -0.0419, -0.0243,  0.0573, -0.0125,  0.0217,\n",
      "          0.0249,  0.0156, -0.0236, -0.0438],\n",
      "        [-0.0562, -0.0655, -0.0054,  0.0555, -0.0204, -0.0300,  0.0148,  0.0211,\n",
      "         -0.0967, -0.0201,  0.0065, -0.0422, -0.0247,  0.0579, -0.0129,  0.0225,\n",
      "          0.0257,  0.0163, -0.0244, -0.0441]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.0278, -0.0322, -0.0026,  0.0274, -0.0099, -0.0146,  0.0073,  0.0104,\n",
      "         -0.0476, -0.0097,  0.0034, -0.0210, -0.0121,  0.0286, -0.0062,  0.0108,\n",
      "          0.0125,  0.0078, -0.0118, -0.0219],\n",
      "        [-0.0281, -0.0328, -0.0027,  0.0278, -0.0102, -0.0150,  0.0074,  0.0105,\n",
      "         -0.0484, -0.0101,  0.0032, -0.0211, -0.0124,  0.0290, -0.0065,  0.0113,\n",
      "          0.0128,  0.0081, -0.0122, -0.0220]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.2204, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f605847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.11] *",
   "language": "python",
   "name": "conda-env-pytorch1.11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
