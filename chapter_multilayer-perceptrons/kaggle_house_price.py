import hashlib
import os.path
import tarfile
import zipfile

import d2l.torch
import numpy
import requests
import pandas as pd
import torch
from matplotlib import pyplot as plt
from torch import nn, log

'''
å®Œæ•´è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ï¼šä¸‹è½½æ•°æ®é›† -> å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼ˆå°†æ•°å€¼æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–(å‡å€¼0ï¼Œæ–¹å·®1)å¤„ç†ï¼Œå°†å­—ç¬¦ä¸²æ•°æ®è¿›è¡Œone-hotç¼–ç ï¼Œå¯¹ç¼ºå¤±æ•°æ®è¿›è¡Œå¤„ç†ï¼‰->åŠ è½½æ•°æ®é›†ï¼Œéœ€è¦å°†æ•°æ®é›†è½¬æ¢ä¸ºtensoræ ¼å¼
-> å®šä¹‰æ¨¡å‹ï¼Œlosså‡½æ•°ï¼Œä¼˜åŒ–å™¨ ->è¿›è¡Œ KæŠ˜äº¤å‰éªŒè¯ ï¼Œæ ¹æ®å¹³å‡è®­ç»ƒè¯¯å·®å’Œå¹³å‡éªŒè¯è¯¯å·®å¤§å°ä»è€Œé€‰æ‹©æ¯”è¾ƒå¥½çš„æ¨¡å‹(ç»“æ„)ä»¥åŠæ¯”è¾ƒå¥½çš„è¶…å‚æ•°(epochs,learning_rate,weight_decay,batch_sizeç­‰)
-> æ ¹æ®é€‰å¥½åçš„æ¨¡å‹å’Œè¶…å‚æ•°å†å¯¹æ•´ä¸ªè®­ç»ƒé›†è¿›è¡Œå…¨éƒ¨è®­ç»ƒï¼Œæ­¤æ—¶ä¸å†æŠŠè®­ç»ƒé›†åˆ†å‡ºä¸€éƒ¨åˆ†ç»™éªŒè¯é›†ï¼Œå¾—åˆ°æœ€åè®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæŒ‡æœ€åå¾—åˆ°çš„æ˜¯è®­ç»ƒå¥½åçš„æ¨¡å‹å‚æ•°ï¼‰-> å°†è®­ç»ƒåçš„æ¨¡å‹ç”¨äºçœŸæ­£æµ‹è¯•é›†ä¸Šé¢è¿›è¡Œé¢„æµ‹
'''
DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'
DATA_HUB['kaggle_house_train'] = (
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')
DATA_HUB['kaggle_house_test'] = (
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")
#ä¸‹è½½æ•°æ®é›†ï¼Œå¹¶è¿”å›ä¸‹è½½æ–‡ä»¶ä½ç½®ï¼Œå¦‚æœæ•°æ®é›†å·²ç»ä¸‹è½½è¿‡ä¸”shalç›¸åŒï¼Œåˆ™ä¸å†é‡å¤è¿›è¡Œä¸‹è½½
def download(name,cache_dir = os.path.join('../data','kaggle_house_data')):
    assert name in DATA_HUB,f"{name}ä¸å­˜åœ¨äº{DATA_HUB}"
    os.makedirs(cache_dir,exist_ok=True)#åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œexist_ok=Trueè¡¨ç¤ºé‡å¤åˆ›å»º,å¹¶è¦†ç›–ä¹‹å‰çš„æ–‡ä»¶å¤¹
    url,shal_hash = DATA_HUB[name]
    fname = os.path.join(cache_dir,url.split('/')[-1])
    if os.path.exists(fname):
        shal = hashlib.sha1()
        with open(fname,'rb') as f :
            while(True):
                data = f.read(1048576)
                if not data :
                    break
                shal.update(data)
            #åˆ¤æ–­shalæ˜¯å¦ç›¸åŒï¼Œç›¸åŒåˆ™ä¸å†é‡æ–°ä¸‹è½½æ•°æ®é›†
            if shal.hexdigest() == shal_hash :
                return fname
    else:
        print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}æ•°æ®é›†....')
        #ä»ç½‘ä¸Šä¸‹è½½æ•°æ®é›†
        data_online = requests.get(url,stream=True,verify=True)
        with open(fname,'wb') as f :
            f.write(data_online.content)
        return fname
#ä¸‹è½½å¹¶è§£å‹ç¼©zip,tar,gzæ–‡ä»¶
def download_extract(name,folder=None):
    fname = download(name)
    #fname = 'D:/Codes/Codes/PycharmCodes/PytorchCodes/D2L-LiMu/train.zip'
    base_dir = os.path.dirname(fname)
    print("base_dir==", base_dir)
    data_dir,ext = os.path.splitext(fname)#extä¸ºæ–‡ä»¶åç¼€å
    print("data_dir==",data_dir)
    print("ext==", ext)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname,'r')
    elif ext in ('.tar','.gz'):
        fp = tarfile.open(fname,'r')
    else:
        assert False,'åªæœ‰zipå’Œtaræ–‡ä»¶æ‰èƒ½è§£å‹ç¼©'
    fp.extractall(base_dir)
    return os.path.join(base_dir,folder) if folder else data_dir
#ä»DATA_HUBä¸­ä¸‹è½½æ‰€æœ‰çš„æ•°æ®é›†
def download_all():
    for name in DATA_HUB:
        download(name)
fname_train = download("kaggle_house_train")
fname_test = download('kaggle_house_test')
train_data = pd.read_csv(fname_train) #pandasè¯»å–csvæ–‡ä»¶ï¼Œtrain_dataæ•°æ®ç±»å‹ä¸ºï¼š<class 'pandas.core.frame.DataFrame'>
print("train_data.type =  ",type(train_data))
test_data = pd.read_csv(fname_test)
print("test_data.shape = ",test_data.shape)
print(train_data.iloc[0:4,[0,1,2,3,-3,-2,-1]]) #æ‰“å°è®­ç»ƒæ•°æ®é›†å‰å››è¡Œé‡Œé¢å‰å››åˆ—å’Œåé¢ä¸‰åˆ—ï¼Œtrain_dataæ•°æ®ç±»å‹ä¸ºï¼š<class 'pandas.core.frame.DataFrame'>
'''
pandas.concat()ï¼špandasæ‹¼æ¥å‡½æ•°
å°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œèƒ½æ›´æ–¹ä¾¿ç”¨äºä¸€èµ·è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œæ‹¼æ¥æ—¶å»æ‰äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç¬¬ä¸€åˆ—ï¼ˆç¼–å·idåˆ—ï¼‰å’Œè®­ç»ƒé›†SalePriceåˆ—ï¼Œ
å› ä¸ºç¼–å·idå¯¹è®­ç»ƒæ•°æ®æ²¡æœ‰ä½œç”¨ï¼Œä¸èƒ½æŠŠç¼–å·idåŠ å…¥è¿›æ¥ï¼Œå¦‚æœåŠ å…¥åè€Œå¯èƒ½ä¼šè®©è®­ç»ƒæ¨¡å‹æ—¶è®°ä½è¿™ä¸ªç¼–å·idå¯¹åº”çš„SalePrice,ä»è€Œæ—¶æµ‹è¯•æ•°æ®é›†æ—¶å˜å¾—æ›´ç³Ÿç³•ï¼Œ
å»æ‰è®­ç»ƒé›†SalePriceåˆ—ï¼Œæ˜¯å› ä¸ºSalePriceåˆ—æ˜¯æ ‡ç­¾labelåˆ—ï¼Œå¿…é¡»å»æ‰è¿™ä¸€åˆ—ï¼Œå¦‚æœåŠ å…¥å°±ä¼šä½¿è®­ç»ƒæ¨¡å‹æå‰çŸ¥é“labelsï¼Œä»è€Œè®­ç»ƒåªä¼šå…³æ³¨è¿™ä¸€åˆ—ï¼Œ
å› æ­¤ä»£ç train_data.iloc[:,1:-1]ä¸­-1å¿…é¡»å«æœ‰ï¼ˆé‡è¦ï¼‰ï¼Œè€Œç”±äºæµ‹è¯•é›†ä¸­æ²¡æœ‰SalePriceåˆ—ï¼Œå› æ­¤ä¸ç”¨å»æ‰ï¼Œå› æ­¤ä»£ç ä¸­test_data.iloc[:,1:]ä¸­æ²¡æœ‰å«æœ‰-1
'''
all_features = pd.concat((train_data.iloc[:,1:-1],test_data.iloc[:,1:]))
print(all_features.iloc[0:4,[0,1,2,3,-3,-2,-1]])
print("all_features.type = ",type(all_features))
#å°†æ‹¼æ¥åçš„æ•°æ®é›†ä¸­æ‰€æœ‰åˆ—çš„å€¼ä¸æ˜¯å­—ç¬¦ä¸²çš„åˆ—é€‰å‡ºæ¥ï¼Œå¹¶å¯¹è¿™äº›æ•°å€¼åˆ—è¿›è¡Œæ•°å€¼æ ‡å‡†åŒ–ï¼Œå°†æ‰€æœ‰ç‰¹å¾æ”¾åœ¨åŒä¸€ä¸ªå…±åŒçš„å°ºåº¦ä¸‹ï¼Œé€šè¿‡å°†æ•°å€¼ç‰¹å¾é‡æ–°ç¼©æ”¾åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥æ ‡å‡†åŒ–æ•°æ®ï¼Œä»è€Œé¿å…æ•°å€¼è¿‡å¤§æ—¶å¸¦æ¥æ¨¡å‹å¯¹å‚æ•°æ±‚å¯¼æ—¶å¯¼æ•°è¿‡å¤§çš„ç°è±¡ï¼Œå¯èƒ½ä¼šå‡ºç°å¯¼æ•°è¾¾åˆ°æ— ç©·å¤§çš„åæœ
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index #å°†æ•°æ®é›†ä¸­æ•°å€¼åˆ—åç§°é€‰å‡ºæ¥
all_features[numeric_features] = all_features[numeric_features].apply(lambda x : (x-x.mean())/x.std()) #æ ¹æ®é€‰å‡ºçš„æ•°å€¼åˆ—åå¯¹è¿™äº›åˆ—è¿›è¡Œæ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
all_features[numeric_features] = all_features[numeric_features].fillna(0) #å°†æ‰€æœ‰ç¼ºå¤±çš„å€¼æ›¿æ¢ä¸ºç›¸åº”ç‰¹å¾çš„å¹³å‡å€¼
print(all_features.iloc[0:4,[0,1,2,3,-3,-2,-1]])
#å¤„ç†å­—ç¬¦ä¸²ç‰¹å¾å€¼ï¼Œé‡‡ç”¨one-hotç¼–ç æ–¹å¼ã€‚
#ä¾‹å¦‚ï¼Œâ€œMSZoningâ€åŒ…å«å€¼â€œRLâ€å’Œâ€œRmâ€ã€‚ æˆ‘ä»¬å°†åˆ›å»ºä¸¤ä¸ªæ–°çš„æŒ‡ç¤ºå™¨ç‰¹å¾â€œMSZoning_RLâ€å’Œâ€œMSZoning_RMâ€ï¼Œå…¶å€¼ä¸º0æˆ–1ã€‚
#æ ¹æ®ç‹¬çƒ­ç¼–ç ï¼Œå¦‚æœâ€œMSZoningâ€çš„åŸå§‹å€¼ä¸ºâ€œRLâ€ï¼Œ åˆ™ï¼šâ€œMSZoning_RLâ€ä¸º1ï¼Œâ€œMSZoning_RMâ€ä¸º0ã€‚ pandasè½¯ä»¶åŒ…ä¼šè‡ªåŠ¨ä¸ºæˆ‘ä»¬å®ç°è¿™ä¸€ç‚¹
all_features = pd.get_dummies(all_features,dummy_na=True)
print(all_features.iloc[:4,:])
n_train = train_data.shape[0] #å¾—åˆ°è®­ç»ƒæ•°æ®é›†çš„ä¸ªæ•°ï¼Œå³è¡Œæ•°
#é¢„å¤„ç†å®Œåï¼Œå°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¼€å¹¶è½¬æ¢æˆtensorç±»å‹
#é€šè¿‡valueså±æ€§ï¼Œå¯ä»¥ä»pandasæ ¼å¼ä¸­æå–NumPyæ ¼å¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡è¡¨ç¤ºç”¨äºè®­ç»ƒ
train_features = torch.tensor(all_features[:n_train].values,dtype=torch.float32)
train_features = train_features.to(device)
test_features = torch.tensor(all_features[n_train:].values,dtype=torch.float32)
test_features = test_features.to(device)
#è·å–è®­ç»ƒé›†çš„labels
train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1,1),dtype=torch.float32)
train_labels = train_labels.to(device)
loss = nn.MSELoss().to(device)#å®šä¹‰å‡æ–¹è¯¯å·®
in_features = train_features.shape[1]#å¾—åˆ°è®­ç»ƒé›†æ¯ä¸ªæ•°æ®çš„ç‰¹å¾ä¸ªæ•°ï¼Œä¹Ÿå³æ˜¯åˆ—æ•°==ç‰¹å¾æ•°
'''
å®šä¹‰æ¨¡å‹ç½‘ç»œï¼š
é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå¸¦æœ‰æŸå¤±å¹³æ–¹çš„çº¿æ€§æ¨¡å‹ã€‚ æ˜¾ç„¶çº¿æ€§æ¨¡å‹å¾ˆç®€å•ï¼Œå®é™…ä¸­æ¨¡å‹æ¯”è¿™ä¸ªå¾ˆå¤æ‚ï¼Œ
ä½†çº¿æ€§æ¨¡å‹æä¾›äº†ä¸€ç§å¥å…¨æ€§æ£€æŸ¥ï¼Œ ä»¥æŸ¥çœ‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚
 å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œä¸èƒ½åšå¾—æ¯”éšæœºçŒœæµ‹æ›´å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¾ˆå¯èƒ½å­˜åœ¨æ•°æ®å¤„ç†é”™è¯¯ã€‚ 
 å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œçº¿æ€§æ¨¡å‹å°†ä½œä¸ºåŸºçº¿ï¼ˆbaselineï¼‰æ¨¡å‹ï¼Œ è®©æˆ‘ä»¬ç›´è§‚åœ°çŸ¥é“æœ€å¥½çš„æ¨¡å‹æœ‰è¶…å‡ºç®€å•çš„æ¨¡å‹å¤šå°‘ã€‚
 å°†çº¿æ€§æ¨¡å‹ä½œä¸ºåŸºå‡†ï¼Œçœ‹å…¶ä»–å¤æ‚çš„æ¨¡å‹æ¯”è¿™ä¸ªçº¿æ€§æ¨¡å‹å¥½å¤šå°‘æˆ–è€…åå¤šå°‘
'''
# def get_net():
#     net = nn.Sequential(nn.Linear(in_features,1))#åªåŒ…å«ä¸€å±‚çº¿æ€§å±‚
#     return net
def get_net():
    net = nn.Sequential(nn.Linear(in_features,128),nn.ReLU(),nn.Linear(128,32),nn.ReLU(),nn.Linear(32,1)).to(device)#åªåŒ…å«ä¸€å±‚çº¿æ€§å±‚
    return net
def log_rmse(net,features,labels):
    # ä¸ºäº†åœ¨å–å¯¹æ•°æ—¶è¿›ä¸€æ­¥ç¨³å®šè¯¥å€¼ï¼Œå°†å°äº1çš„å€¼è®¾ç½®ä¸º1,å› ä¸ºå¯¹è¶‹è¿‘äº0å–å¯¼æ•°æ—¶ä¼šè¶‹äºè´Ÿæ— ç©·å¤§
    clipped_preds = torch.clamp(net(features),1,float('inf'))
    # å¯¹é¢„æµ‹ç»“æœå’ŒçœŸå®labelså–å¯¹æ•°ï¼Œæ¯”è¾ƒä»–ä»¬çš„ç›¸å¯¹è¯¯å·®y^/yå¤§äº1æˆ–è€…å°äº1ï¼Œå†å–å¯¹æ•°åˆ¤æ–­ä»–ä»¬çš„è¯¯å·®å¤§å°ï¼Œå³log(y^/y)=log(y^)-log(y)
    rmse_loss = torch.sqrt(loss(log(clipped_preds),log(labels))).to(device)
    return rmse_loss.item()
# è®­ç»ƒæ¨¡å‹
def train(net,train_datas,train_labels,test_datas,test_labels,batch_size,epochs,learning_rate,weight_decay):
    train_ls = []
    test_ls = []
    train_iter = d2l.torch.load_array((train_datas,train_labels),batch_size)#åŠ è½½æ•°æ®
    optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)#Adamä¼˜åŒ–ç®—æ³•ï¼Œå¯¹åˆå§‹å­¦ä¹ ç‡ä¸é‚£ä¹ˆæ•æ„Ÿ
    for epoch in range(epochs):
        for X,y in train_iter:
            X = X.to(device)
            y = y.to(device)
            optimizer.zero_grad()
            l = loss(net(X),y) #è®¡ç®—é¢„æµ‹å€¼å’ŒçœŸå®å€¼æŸå¤±å¤§å°ï¼Œæ•°æ®é›†æŸå¤±å¹³æ–¹å’Œå†æ±‚å‡å€¼ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
            l.backward() #è®¡ç®—æ¢¯åº¦
            optimizer.step() #æ›´æ–°å‚æ•°
        # æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨å‡æ–¹è¯¯å·®è®¡ç®—ï¼Œå½“æ¨¡å‹è®­ç»ƒå®Œåè®¡ç®—æŸå¤±æ—¶ä½¿ç”¨rmse()ç›¸å¯¹æŸå¤±è®¡ç®—ï¼Œ
        # å› ä¸ºå¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨ç›¸å¯¹æŸå¤±è®¡ç®—æ—¶ç”±äºæœ‰å¼€æ ¹å·ï¼Œlogï¼Œæ±‚å¯¼æ•°æ—¶ä¼šå¾ˆå¤æ‚ï¼Œå¾ˆå®¹æ˜“é€ æˆæ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¢¯åº¦æ¶ˆå¤±ï¼Œå› æ­¤è®­ç»ƒæ¨¡å‹æ—¶é‡‡ç”¨å‡æ–¹è¯¯å·®æ¥è¿›è¡Œè®¡ç®—
        train_ls.append(log_rmse(net,train_datas,train_labels))#è®¡ç®—æ¯è½®è®­ç»ƒå®Œååœ¨è®­ç»ƒé›†ä¸Šé¢çš„æŸå¤±
        if test_labels is not None:
            test_ls.append(log_rmse(net,test_datas,test_labels))#è®¡ç®—æ¯è½®è®­ç»ƒå®Œååœ¨éªŒè¯é›†ä¸Šé¢çš„æŸå¤±
    return train_ls,test_ls
#é‡‡ç”¨kæŠ˜äº¤å‰éªŒè¯ï¼Œæœ‰åŠ©äºæ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°é€‰æ‹©ï¼Œé€‰æ‹©ç¬¬ ğ‘– ä¸ªåˆ‡ç‰‡ä½œä¸ºéªŒè¯æ•°æ®ï¼Œå…¶ä½™éƒ¨åˆ†ä½œä¸ºè®­ç»ƒæ•°æ®
def get_k_fold_data(k,i,X,y):
    assert k>1
    fold_size = X.shape[0] // k #æ¯ä¸ªåˆ‡ç‰‡ä¸ªæ•°ï¼Œæ³¨æ„è®­ç»ƒé›†ä¸èƒ½æ•´é™¤æƒ…å†µ
    X_train,y_train = None,None
    for j in range(k):
        idx = slice(j*fold_size,(j+1)*fold_size)
        X_part,y_part = X[idx,:],y[idx]
        if j == i:
            X_valid,y_valid = X_part,y_part #é€‰æ‹©ç¬¬ ğ‘– ä¸ªåˆ‡ç‰‡ä½œä¸ºéªŒè¯æ•°æ®
        else:
            if X_train == None:
                X_train = X_part
                y_train = y_part
            else:
                X_train = torch.cat([X_train,X_part],0) #å…¶ä½™éƒ¨åˆ†ä½œä¸ºè®­ç»ƒæ•°æ®
                y_train = torch.cat([y_train,y_part],0)
    return X_train,y_train,X_valid,y_valid
#åœ¨KæŠ˜äº¤å‰éªŒè¯ä¸­è®­ç»ƒKæ¬¡åï¼Œè¿”å›è®­ç»ƒå’ŒéªŒè¯è¯¯å·®çš„å¹³å‡å€¼ï¼Œæ¥è¡¨ç¤ºå¯¹è¿™ä¸ªæ•°æ®é›†çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†è¯¯å·®å¤§å°æ˜¯å¤šå°‘ï¼Œæœ‰å¯èƒ½æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯æ—¶å¯¹æŸä¸€ä¸ªåˆ‡ç‰‡æ•°æ®é›†æ¯”è¾ƒæ•æ„Ÿï¼Œå¯¼è‡´è¯¯å·®å°ï¼Œå¦ä¸€ä¸ªåˆ‡ç‰‡æ•°æ®é›†ä¸æ•æ„Ÿï¼Œå¯¼è‡´è¯¯å·®å˜å¤§
def k_fold(k,train_datas,train_labels,batch_size,epochs,learning_rate,weight_decay):
    train_ls_sum = 0
    valid_ls_sum = 0
   # net = get_net()
    for i in range(k):
        # æ³¨æ„ï¼š(é‡è¦) æ¯ä¸€æ¬¡è¿›è¡Œå¾ªç¯æ—¶éƒ½è¦é‡æ–°åˆå§‹åŒ–ç½‘ç»œæ¨¡å‹ï¼Œé‡æ–°å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä¸èƒ½ä½¿ç”¨ä¸Šä¸€æ¬¡è®­ç»ƒå·²ç»ç”¨äºè®­ç»ƒçš„æ¨¡å‹ï¼Œå› ä¸ºæ¯ä¸€æŠ˜è¡¨æ˜è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸åŒï¼Œ
        # ä¸”å¦‚æœä½¿ç”¨ä¸Šä¸€æ¬¡å¾ªç¯è®­ç»ƒåçš„ç½‘ç»œæ¥è®­ç»ƒå’ŒéªŒè¯è¿™ä¸€æŠ˜çš„æ•°æ®é›†æ—¶ï¼Œç”±äºæ¨¡å‹å·²ç»åœ¨ä¸Šä¸€æ¬¡å¾ªç¯ä¸­çœ‹è¿‡éƒ¨åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†æ—¶ï¼Œä»è€Œå¯¼è‡´éªŒè¯é›†è¯¯å·®å˜å°ï¼Œå¯¼è‡´æ¨¡å‹é¢„æµ‹ä¸å‡†ç¡®ï¼Œå› æ­¤ä¼šä½¿æ¨¡å‹åœ¨çœŸæ­£æµ‹è¯•é›†ä¸Šé¢é¢„æµ‹æ—¶è¯¯å·®ä¼šåå¤§
        net = get_net() #ä¸èƒ½æŠŠæ¨¡å‹åˆå§‹åŒ–æ”¾åœ¨ç¬¬157è¡Œä»£ç å¤„(å¾ªç¯å¤–é¢)è¿›è¡Œåˆå§‹åŒ–ï¼Œå¿…é¡»æ”¾åœ¨å¾ªç¯å†…è¿›è¡Œåˆå§‹åŒ– ----é‡è¦
        data = get_k_fold_data(k,i,train_datas,train_labels)
        #åœ¨iæŠ˜ä¸Šå¯¹æ•°æ®é›†è®­ç»ƒepochè½®å¾—å‡ºçš„å¯¹å½“å‰è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šé¢çš„è¯¯å·®å¤§å°ï¼Œæ¯”å¦‚å°†ç¬¬ä¸€ä¸ªåˆ‡ç‰‡ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä»–æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œåˆ™æ”¾å…¥æ¨¡å‹è®­ç»ƒä¸­ï¼Œè®­ç»ƒepochè½®åå¾—å‡ºåœ¨æ¯è½®è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šé¢çš„è¯¯å·®å¤§å°
        train_ls,valid_ls = train(net,*data,batch_size=batch_size,epochs=epochs,learning_rate=learning_rate,weight_decay=weight_decay)
        train_ls_sum += train_ls[-1] #å¾—åˆ°é™¤å¼€ç¬¬iä¸ªåˆ‡ç‰‡éªŒè¯é›†ä»¥å¤–çš„è®­ç»ƒæ•°æ®é›†åœ¨æœ€åä¸€è½®çš„è¯¯å·®å¤§å°
        valid_ls_sum += valid_ls[-1]#å¾—åˆ°ç¬¬iä¸ªåˆ‡ç‰‡éªŒè¯é›†åœ¨æœ€åä¸€è½®çš„è¯¯å·®å¤§å°
        if i == 1:
            d2l.torch.plot(list(range(1, epochs + 1)), [train_ls, valid_ls],
                     xlabel='epoch', ylabel='rmse', xlim=[1, epochs],
                     legend=['train', 'valid'], yscale='log')
            plt.show()
        print(f'{i+1}æŠ˜ï¼Œè®­ç»ƒlog_rmse{float(train_ls[-1]):f},éªŒè¯log_rmse{float(valid_ls[-1]):f}')
        # print(f'æŠ˜{i + 1}ï¼Œè®­ç»ƒlog rmse{float(train_ls[-1]):f}, '
        #       f'éªŒè¯log rmse{float(valid_ls[-1]):f}')
    return train_ls_sum / k,valid_ls_sum / k
#æ ¹æ®KæŠ˜äº¤å‰éªŒè¯å¾—åˆ°å¹³å‡è®­ç»ƒè¯¯å·®å’Œå¹³å‡éªŒè¯è¯¯å·®ï¼Œä»è€Œåˆ¤æ–­å½“å‰æ¨¡å‹å’Œè¶…å‚æ•°æ˜¯å¦å¥½åï¼Œä»è€Œè¿›è¡Œæ¨¡å‹å’Œè¶…å‚æ•°é€‰æ‹©
k,num_epochs,lr,weight_decay,batch_size = 5,1000,0.3,25000,64
train_loss,valid_loss = k_fold(k,train_features,train_labels,batch_size,num_epochs,lr,weight_decay)
print(f'{k}æŠ˜ï¼Œå¹³å‡è®­ç»ƒlog_rms{float(train_loss):f},å¹³å‡éªŒè¯log_rmse{float(valid_loss):f}')
#æ ¹æ®é€‰æ‹©å‡ºæ¯”è¾ƒå¥½çš„æ¨¡å‹å’Œè¶…å‚æ•°åå†å¯¹è®­ç»ƒé›†è¿›è¡Œå…¨éƒ¨ç”¨äºè®­ç»ƒï¼Œæ­¤æ—¶ä¸ç”¨åœ¨è®­ç»ƒé›†ä¸Šé¢åˆ†å‡ºä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†ï¼Œå°†è®­ç»ƒå¥½åçš„æ¨¡å‹åŒ…å«æ¨¡å‹é‡Œé¢çš„å‚æ•°ç”¨äºçœŸæ­£çš„æµ‹è¯•é›†ä¸Šé¢éªŒè¯
def train_pred(train_features,train_labels,test_features,test_data,batch_size,epochs,learning_rate,weight_decay):
    net = get_net()
    #å°†è®­ç»ƒé›†å…¨éƒ¨ç”¨äºè®­ç»ƒï¼Œå¾—åˆ°æœ€åè®­ç»ƒå¥½çš„æ¨¡å‹
    train_ls,_ = train(net,train_features,train_labels,None,None,batch_size=batch_size,epochs=epochs,learning_rate=learning_rate,weight_decay=weight_decay)
    d2l.torch.plot(numpy.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    #åœ¨matlibä¸Šé¢æ˜¾ç¤ºå‡ºæ¥
    #plt.show()
    print("è®­ç»ƒå®Œå…¨éƒ¨æ•°æ®åçš„loss = ",train_ls[-1])
    #å°†è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨äºæµ‹è¯•é›†ä¸Šé¢è¿›è¡Œé¢„æµ‹å¾—åˆ°æµ‹è¯•é›†ä¸Šé¢æ¯ä¸ªæˆ¿å­ä»·æ ¼
    pred_price = net(test_features).detach().numpy()
    test_data['SalePrice'] = pd.Series(pred_price.reshape(1,-1)[0])
    #ä½¿ç”¨pandas.cancat()å‡½æ•°è¿›è¡Œåˆ—æ‹¼æ¥ï¼Œaxis=1è¡¨ç¤ºåˆ—æ‹¼æ¥
    submission = pd.concat([test_data['Id'],test_data['SalePrice']],axis=1)
    #å°†æ‹¼æ¥ç»“æœè¾“å‡ºåˆ°csvæ–‡ä»¶ä¸­ï¼Œsubmissionç±»å‹ä¸ºï¼š<class 'pandas.core.frame.DataFrame'>
    submission.to_csv("../data/kaggle_house_data/sumission",index=False)
train_pred(train_features,train_labels,test_features,test_data,batch_size,num_epochs,lr,weight_decay)




